{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction   \n",
    "1. Recommandation environment: Anaconda virtual environment (python 3.6).\n",
    "2. Install tensorflow 1.8.0 (newest version is 1.9.0). \n",
    "3. Install all the other package. Install feature_selector for https://github.com/WillKoehrsen/feature-selector. (put it in current directory).\n",
    "4. Dataset from Kaggle competition https://www.kaggle.com/c/home-credit-default-risk/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "#download feature_selector package from https://github.com/WillKoehrsen/feature-selector\n",
    "from feature_selector import FeatureSelector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = \"./dataset/home_credit/sources\"\n",
    "OUT_PATH = \"./dataset/home_credit/outputs\"\n",
    "print(os.listdir(SRC_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan values\n",
    "def fillna_df(df, verbose=False):\n",
    "    cat_cols_object = df.dtypes[df.dtypes == 'object'].index\n",
    "    cat_cols_int = df.dtypes[df.dtypes == 'int64'].index\n",
    "    numeric_cols = df.dtypes[df.dtypes == 'float64'].index\n",
    "    if verbose:\n",
    "        display(get_misstable(df[cat_cols_object]))\n",
    "        display(get_misstable(df[cat_cols_int]))\n",
    "        display(get_misstable(df[numeric_cols]))\n",
    "    df[cat_cols_object] = df[cat_cols_object].fillna('etc')\n",
    "    df[cat_cols_int] = df[cat_cols_int].fillna(0)\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)  \n",
    "    return df, cat_cols_object, cat_cols_int, numeric_cols\n",
    "\n",
    "def cross_validation(df,train_labels):\n",
    "    from sklearn.model_selection import train_test_split \n",
    "    train_X, val_X, train_y, val_y = train_test_split(df, train_labels, test_size=0.25, random_state=42)\n",
    "    return train_X, val_X, train_y, val_y\n",
    "\n",
    "\n",
    "# def normalize_age(data):\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "#     data[\"Age\"] = scaler.fit_transform(data[\"Age\"].values.reshape(-1,1))\n",
    "#     return data\n",
    "# train_data = normalize_age(train_data)\n",
    "# test_data = normalize_age(test_data)\n",
    "# train_data.head()\n",
    "\n",
    "def separate_columns(df):\n",
    "    cate_columns = []\n",
    "    num_columns = []\n",
    "    #separate columns\n",
    "    for column in df.columns:\n",
    "        if column in list(df.select_dtypes(include=['object']).columns):\n",
    "            cate_columns.append(column)\n",
    "        if column in list(df.select_dtypes(exclude=['object']).columns):\n",
    "            num_columns.append(column)\n",
    "    return cate_columns,num_columns\n",
    "\n",
    "\n",
    "def conv_feature_columns(df):\n",
    "    cate_columns, num_columns=separate_columns(df)\n",
    "    tf_num_feature_column=[]\n",
    "    tf_cate_feature_column=[]\n",
    "    for column in num_columns:\n",
    "        column_name =str(column)\n",
    "        column_name = tf.feature_column.numeric_column(column)\n",
    "        tf_num_feature_column.append(column_name)\n",
    "    for column in cate_columns:\n",
    "        column_name =str(column)\n",
    "        vocabulary_list_c=df[column].unique().tolist()\n",
    "        column_name = tf.feature_column.categorical_column_with_vocabulary_list(column,vocabulary_list_c)\n",
    "        tf_cate_feature_column.append(column_name)     \n",
    "    #hashing from categories to numerical use API \n",
    "    #transformation using bucketized for numerical to categories use API\n",
    "    return tf_num_feature_column,tf_cate_feature_column\n",
    "\n",
    "def indicator_deep_column(tf_cate_feature_column):\n",
    "    tf_cate_feature_column_indicator=[]\n",
    "    for column in tf_cate_feature_column:\n",
    "        column_indicator=tf.feature_column.indicator_column(column)\n",
    "        tf_cate_feature_column_indicator.append(column_indicator)\n",
    "    return tf_cate_feature_column_indicator\n",
    "\n",
    "# def cross_feature_selection():\n",
    "#     return \n",
    "\n",
    "def wide_deep_columns(df):\n",
    "    tf_num_feature_column,tf_cate_feature_column=conv_feature_columns(df)\n",
    "    deep_column_indicator_part = indicator_deep_column(tf_cate_feature_column) \n",
    "    #categories in base_column\n",
    "    base_column = tf_cate_feature_column\n",
    "    #categories types with 0.3-0.7 cor\n",
    "    crossed_column = []\n",
    "    wide_column = []\n",
    "    deep_column = []    \n",
    "    wide_column = base_column + crossed_column\n",
    "    deep_column = tf_num_feature_column + deep_column_indicator_part \n",
    "    return wide_column,deep_column\n",
    "\n",
    "\n",
    "def grid_selection(train,train_labels):\n",
    "    fs = FeatureSelector(data = train, labels = train_labels)\n",
    "    fs.identify_all(selection_params = {'missing_threshold': 0.6, 'correlation_threshold': 0.80, \n",
    "                                    'task': 'classification', 'eval_metric': 'auc', \n",
    "                                     'cumulative_importance': 0.99})\n",
    "    train_removed_all_once = fs.remove(methods = 'all', keep_one_hot = False)\n",
    "    fs.feature_importances.head()\n",
    "    fs.record_collinear.head() \n",
    "    return train_removed_all_once\n",
    "\n",
    "def get_model():\n",
    "    return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 75, 50, 25],\n",
    "        linear_optimizer = tf.train.FtrlOptimizer(learning_rate=0.00001,l1_regularization_strength=0.005,l2_regularization_strength=0.001),\n",
    "        dnn_optimizer=tf.train.ProximalAdagradOptimizer(0.00001,initial_accumulator_value=0.1,l1_regularization_strength=0.005,l2_regularization_strength=0.001))\n",
    "\n",
    "def get_model2():\n",
    "    return tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 75, 50, 25],\n",
    "    linear_optimizer = tf.train.FtrlOptimizer(learning_rate=0.00001,l1_regularization_strength=0.005,l2_regularization_strength=0.001),\n",
    "    dnn_optimizer=tf.train.ProximalAdagradOptimizer(0.00001,initial_accumulator_value=0.1,l1_regularization_strength=0.005,l2_regularization_strength=0.001)\n",
    ")\n",
    "\n",
    "#partial_fit API\n",
    "# def get_model():\n",
    "#     return tf.contrib.learn.DNNLinearCombinedEstimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and preprocessing and get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "train = pd.read_csv(SRC_PATH + '/application_train.csv')\n",
    "test= pd.read_csv(SRC_PATH + '/application_test.csv')\n",
    "train_labels = train['TARGET']\n",
    "#y_df = pd.Series(y, index=X.index)\n",
    "\n",
    "#drop label and user Id columns\n",
    "train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "\n",
    "#preprocessing \n",
    "#train_removed_all_once = grid_selection(train,train_labels)\n",
    "df, cat_cols_object, cat_cols_int, numeric_cols=fillna_df(train_removed_all_once, verbose=False)\n",
    "\n",
    "df_scale=df\n",
    "scaler = StandardScaler().fit(df_scale[numeric_cols])\n",
    "df_scale.loc[:,numeric_cols] = scaler.transform(df_scale[numeric_cols])\n",
    "\n",
    "#get columns\n",
    "wide_columns,deep_columns = wide_deep_columns(df_scale)\n",
    "\n",
    "#train and validation separate\n",
    "train_X, val_X, train_y, val_y = cross_validation(df_scale,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir ='./widendeep3'\n",
    "\n",
    "run_config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "                                    save_checkpoints_secs=300,\n",
    "                                    keep_checkpoint_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = tf.estimator.DNNLinearCombinedClassifier(linear_feature_columns=wide_columns, dnn_feature_columns=deep_columns,\n",
    "#                                                     dnn_hidden_units=[100, 75, 50,25], dnn_activation_fn=tf.nn.relu,\n",
    "#                                                     dnn_dropout=0.5, config=run_config)       \n",
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 75, 50,25],\n",
    "        config=run_config,\n",
    "        linear_optimizer = tf.train.FtrlOptimizer(learning_rate=0.00001,l1_regularization_strength=0.005,l2_regularization_strength=0.001),\n",
    "        dnn_optimizer=tf.train.ProximalAdagradOptimizer(0.00001,initial_accumulator_value=0.1,l1_regularization_strength=0.005,l2_regularization_strength=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(train_X, train_y, batch_size = 128, num_threads=3, shuffle=True)\n",
    "eval_input_fn = tf.estimator.inputs.pandas_input_fn(val_X, val_y, batch_size = 5000, shuffle=False)\n",
    "pred_input_fn = tf.estimator.inputs.pandas_input_fn(val_X, val_y, batch_size = len(val_X), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=None, hooks=None)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=10, start_delay_secs=240, throttle_secs=600,\n",
    "                                  exporters=None, hooks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "train = pd.read_csv(SRC_PATH + '/application_train.csv')\n",
    "test= pd.read_csv(SRC_PATH + '/application_test.csv')\n",
    "train_labels = train['TARGET']\n",
    "\n",
    "#drop label and user Id columns\n",
    "train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "\n",
    "#preprocessing \n",
    "train_removed_all_once = grid_selection(train,train_labels)\n",
    "df, cat_cols_object, cat_cols_int, numeric_cols=fillna_df(train_removed_all_once, verbose=False)\n",
    "\n",
    "#get columns\n",
    "wide_columns,deep_columns = wide_deep_columns(df)\n",
    "\n",
    "# print(\"this is wide column\")\n",
    "# print(wide_columns)\n",
    "# print(\"this is deep column\")\n",
    "# print(deep_columns)\n",
    "\n",
    "df2=df\n",
    "input_fn_train = tf.estimator.inputs.pandas_input_fn(\n",
    "    df1,\n",
    "    batch_size=128,\n",
    "    num_epochs=10,\n",
    "    shuffle= False,\n",
    "    queue_capacity=1000,\n",
    "    num_threads=1,\n",
    "    target_column='target'\n",
    ")\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])\n",
    "\n",
    "#m.train(input_fn=input_fn_train, steps=100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #run the model\n",
    "# model=get_model()\n",
    "\n",
    "# train_X, val_X, train_y, val_y = cross_validation(df,train_labels)\n",
    "\n",
    "# model.train(train_X, train_y)\n",
    "\n",
    "# model.fit()\n",
    "# model.partial_fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df\n",
    "df1['target'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtpye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
