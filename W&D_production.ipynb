{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction   \n",
    "1. Recommandation environment: Anaconda virtual environment (python 3.6).\n",
    "2. Install tensorflow 1.8.0 (newest version is 1.9.0). \n",
    "3. Install all the other package. Install feature_selector for https://github.com/WillKoehrsen/feature-selector. (put it in current directory).\n",
    "4. Dataset from Kaggle competition https://www.kaggle.com/c/home-credit-default-risk/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "#download feature_selector package from https://github.com/WillKoehrsen/feature-selector\n",
    "from feature_selector import FeatureSelector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = \"./dataset/home_credit/sources\"\n",
    "OUT_PATH = \"./dataset/home_credit/outputs\"\n",
    "print(os.listdir(SRC_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan values\n",
    "def fillna_df(df, verbose=False):\n",
    "    cat_cols_object = df.dtypes[df.dtypes == 'object'].index\n",
    "    cat_cols_int = df.dtypes[df.dtypes == 'int64'].index\n",
    "    numeric_cols = df.dtypes[df.dtypes == 'float64'].index\n",
    "    if verbose:\n",
    "        display(get_misstable(df[cat_cols_object]))\n",
    "        display(get_misstable(df[cat_cols_int]))\n",
    "        display(get_misstable(df[numeric_cols]))\n",
    "    df[cat_cols_object] = df[cat_cols_object].fillna('etc')\n",
    "    df[cat_cols_int] = df[cat_cols_int].fillna(0)\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)  \n",
    "    return df, cat_cols_object, cat_cols_int, numeric_cols\n",
    "\n",
    "def cross_validation(df,train_labels):\n",
    "    from sklearn.model_selection import train_test_split \n",
    "    train_X, val_X, train_y, val_y = train_test_split(df, train_labels, test_size=0.25, random_state=42)\n",
    "    return train_X, val_X, train_y, val_y\n",
    "\n",
    "def separate_columns(df):\n",
    "    cate_columns = []\n",
    "    num_columns = []\n",
    "    #separate columns\n",
    "    for column in df.columns:\n",
    "        if column in list(df.select_dtypes(include=['object']).columns):\n",
    "            cate_columns.append(column)\n",
    "        if column in list(df.select_dtypes(exclude=['object']).columns):\n",
    "            num_columns.append(column)\n",
    "    return cate_columns,num_columns\n",
    "\n",
    "def conv_feature_columns(df):\n",
    "    cate_columns, num_columns=separate_columns(df)\n",
    "    tf_num_feature_column=[]\n",
    "    tf_cate_feature_column=[]\n",
    "    for column in num_columns:\n",
    "        column_name =str(column)\n",
    "        column_name = tf.feature_column.numeric_column(column)\n",
    "        tf_num_feature_column.append(column_name)\n",
    "    for column in cate_columns:\n",
    "        column_name =str(column)\n",
    "        vocabulary_list_c=df[column].unique().tolist()\n",
    "        column_name = tf.feature_column.categorical_column_with_vocabulary_list(column,vocabulary_list_c)\n",
    "        tf_cate_feature_column.append(column_name)     \n",
    "    #hashing from categories to numerical use API \n",
    "    #transformation using bucketized for numerical to categories use API\n",
    "    return tf_num_feature_column,tf_cate_feature_column\n",
    "\n",
    "def indicator_deep_column(tf_cate_feature_column):\n",
    "    tf_cate_feature_column_indicator=[]\n",
    "    for column in tf_cate_feature_column:\n",
    "        column_indicator=tf.feature_column.indicator_column(column)\n",
    "        tf_cate_feature_column_indicator.append(column_indicator)\n",
    "    return tf_cate_feature_column_indicator\n",
    "\n",
    "def cross_feature_selection(df):\n",
    "    crossed_col=[]\n",
    "    #1\n",
    "    DAYS_BIRTH_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DAYS_BIRTH'),\n",
    "                                               boundaries = [-25000,-20000,-15000,-10000])\n",
    "    CNT_CHILDREN_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('CNT_CHILDREN'),\n",
    "                                                 boundaries = [2,4,6,8,10])\n",
    "    crossed_col_1 = tf.feature_column.crossed_column( [DAYS_BIRTH_c, CNT_CHILDREN_c], 5000)\n",
    "    crossed_col.append(crossed_col_1) \n",
    "    #2\n",
    "    DEF_30_CNT_SOCIAL_CIRCLE_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DEF_30_CNT_SOCIAL_CIRCLE'),\n",
    "                                                             boundaries = [2,4,6,8])\n",
    "    OBS_30_CNT_SOCIAL_CIRCLE_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('OBS_30_CNT_SOCIAL_CIRCLE'),\n",
    "                                                 boundaries = [5, 10,15,20,25,30])\n",
    "    crossed_col_2 = tf.feature_column.crossed_column([DEF_30_CNT_SOCIAL_CIRCLE_c,OBS_30_CNT_SOCIAL_CIRCLE_c], 5000)\n",
    "    crossed_col.append(crossed_col_2) \n",
    "#     #5 too much missing value\n",
    "#     YEARS_BUILD_AVG_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('YEARS_BUILD_AVG'),\n",
    "#                                                boundaries = [0,0.2,0.4,0.6,0.8])\n",
    "#     APARTMENTS_AVG_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('APARTMENTS_AVG'),\n",
    "#                                                  boundaries = [0,0.2,0.4,0.6,0.8])\n",
    "#     crossed_col_5 = tf.feature_column.crossed_column( [YEARS_BUILD_AVG_c, APARTMENTS_AVG_c], 5000)\n",
    "#     crossed_col.append(crossed_col_5) \n",
    "#     #3\n",
    "#     DEF_30_CNT_SOCIAL_CIRCLE_c2=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DEF_30_CNT_SOCIAL_CIRCLE'),\n",
    "#                                                             boundaries = [2,4,6,8])\n",
    "#     OBS_60_CNT_SOCIAL_CIRCLE_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('OBS_60_CNT_SOCIAL_CIRCLE'),\n",
    "#                                                  boundaries = [5, 10,15,20,25,30])\n",
    "#     crossed_col_3 = tf.feature_column.crossed_column([DEF_30_CNT_SOCIAL_CIRCLE_c2,OBS_30_CNT_SOCIAL_CIRCLE_c], 5000)\n",
    "#     crossed_col.append(crossed_col_3) \n",
    "#     #4\n",
    "#     DAYS_REGISTRATION_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DAYS_REGISTRATION'),\n",
    "#                                                boundaries = [-25000,-20000,-15000,-10000,-5000,0])\n",
    "#     DAYS_BIRTH_c2=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DAYS_BIRTH'),\n",
    "#                                                boundaries = [-25000,-20000,-15000,-10000])\n",
    "#     crossed_col_4 = tf.feature_column.crossed_column( [DAYS_BIRTH_c2, DAYS_REGISTRATION_c], 5000)\n",
    "#     crossed_col.append(crossed_col_4) \n",
    "#     crossed_col = crossed_col_1 + crossed_col_2 + crossed_col_3 + crossed_col_4\n",
    "    return crossed_col\n",
    "\n",
    "def wide_deep_columns(df):\n",
    "    tf_num_feature_column,tf_cate_feature_column=conv_feature_columns(df)\n",
    "    deep_column_indicator_part = indicator_deep_column(tf_cate_feature_column) \n",
    "    #categories in base_column\n",
    "    base_column = tf_cate_feature_column\n",
    "    #categories types with 0.3-0.7 cor\n",
    "    crossed_column = []\n",
    "    wide_column = []\n",
    "    deep_column = []    \n",
    "    crossed_column=cross_feature_selection(df)\n",
    "    wide_column = base_column + crossed_column\n",
    "    deep_column = tf_num_feature_column + deep_column_indicator_part \n",
    "    return wide_column,deep_column\n",
    "\n",
    "\n",
    "def grid_selection(train,train_labels):\n",
    "    fs = FeatureSelector(data = train, labels = train_labels)\n",
    "    fs.identify_all(selection_params = {'missing_threshold': 0.6, 'correlation_threshold': 0.95, \n",
    "                                    'task': 'classification', 'eval_metric': 'auc', \n",
    "                                     'cumulative_importance': 0.99})\n",
    "    train_removed_all_once = fs.remove(methods = 'all', keep_one_hot = False)\n",
    "    fs.feature_importances.head()\n",
    "    fs.record_collinear.head()\n",
    "    return train_removed_all_once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and preprocessing and get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "train = pd.read_csv(SRC_PATH + '/application_train.csv')\n",
    "test= pd.read_csv(SRC_PATH + '/application_test.csv')\n",
    "\n",
    "train_labels = train['TARGET']\n",
    "#y_df = pd.Series(y, index=X.index)\n",
    "\n",
    "#drop label and user Id columns\n",
    "train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "\n",
    "#preprocessing \n",
    "train_removed_all_once = grid_selection(train,train_labels)\n",
    "df, cat_cols_object, cat_cols_int, numeric_cols=fillna_df(train_removed_all_once, verbose=False)\n",
    "\n",
    "# test.columns = X.columns\n",
    "\n",
    "#scaling\n",
    "df_scale=df\n",
    "scale_column=df_scale.select_dtypes(exclude=['object']).columns\n",
    "scaler = StandardScaler().fit(df_scale[scale_column])\n",
    "df_scale.loc[:,scale_column] = scaler.transform(df_scale[scale_column])\n",
    "\n",
    "#get columns\n",
    "wide_columns,deep_columns = wide_deep_columns(df_scale)\n",
    "\n",
    "#train and validation separate\n",
    "train_X, val_X, train_y, val_y = cross_validation(df_scale,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir ='./widendeep8'\n",
    "\n",
    "run_config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "                                    save_checkpoints_secs=300,\n",
    "                                    keep_checkpoint_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNLinearCombinedClassifier(linear_feature_columns=wide_columns, dnn_feature_columns=deep_columns,\n",
    "                                                    dnn_hidden_units=[500,150,50], dnn_activation_fn=tf.nn.relu,\n",
    "                                                    dnn_dropout=0.5,config=run_config)  \n",
    "\n",
    "# estimator = tf.estimator.DNNLinearCombinedClassifier(linear_feature_columns=wide_columns, dnn_feature_columns=deep_columns,\n",
    "#                                                     dnn_hidden_units=[500,250,150,50], dnn_activation_fn=tf.nn.relu,\n",
    "#                                                     linear_optimizer = tf.train.FtrlOptimizer(0.0001,l1_regularization_strength=0.005),\n",
    "#                                                     dnn_dropout=0.5,config=run_config)    \n",
    "# estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "#         model_dir=model_dir,\n",
    "#         linear_feature_columns=wide_columns,\n",
    "#         dnn_feature_columns=deep_columns,\n",
    "#         dnn_hidden_units=[100, 75, 50,25],\n",
    "#         config=run_config,\n",
    "#         linear_optimizer = tf.train.FtrlOptimizer(learning_rate=0.0001,l1_regularization_strength=0.005,l2_regularization_strength=0.001),\n",
    "#         dnn_optimizer=tf.train.ProximalAdagradOptimizer(0.000 1,initial_accumulator_value=0.1,l1_regularization_strength=0.005,l2_regularization_strength=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(train_X, train_y, batch_size = 128, num_threads=3, shuffle=True)\n",
    "eval_input_fn = tf.estimator.inputs.pandas_input_fn(val_X, val_y, batch_size = 5000, shuffle=False)\n",
    "pred_input_fn = tf.estimator.inputs.pandas_input_fn(val_X, val_y, batch_size = len(val_X), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=10000, hooks=None)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=10, start_delay_secs=240, throttle_secs=600,\n",
    "                                  exporters=None, hooks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = estimator.evaluate(input_fn=pred_input_fn, steps=1)\n",
    "print()\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"# Valid Measures: {}\".format(valid_results))\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.estimator.prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on crossed feature columns selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureSelector(data = train, labels = train_labels)\n",
    "fs.identify_all(selection_params = {'missing_threshold': 0.6, 'correlation_threshold': 0.3, \n",
    "                                    'task': 'classification', 'eval_metric': 'auc', \n",
    "                                     'cumulative_importance': 0.99})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value < 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_missing(missing_threshold = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=fs.missing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df_above=miss_df[miss_df['missing_fraction']<0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#miss_df_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance Top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs.feature_importances.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs.record_collinear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_importance=fs.feature_importances.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fea_importance=fea_importance['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_fea_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs.record_collinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select corr 0.3-0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_cross = fs.record_collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_pre_cross[df_pre_cross['corr_value']>=0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2[df2['corr_value']<=0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df3[df3['drop_feature'].isin(list_fea_importance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5=df4[df4['drop_feature'].isin(list_fea_importance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_fea_A=df4['drop_feature']\n",
    "crossed_fea_B=df4['corr_feature'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat crossed column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS_BUILD_AVG_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('YEARS_BUILD_AVG'),\n",
    "                                               boundaries = [0,0.2,0.4,0.6,0.8])\n",
    "APARTMENTS_AVG_c=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('APARTMENTS_AVG'),\n",
    "                                                 boundaries = [0,0.2,0.4,0.6,0.8])\n",
    "crossed_col_5 = tf.feature_column.crossed_column( [YEARS_BUILD_AVG_c, APARTMENTS_AVG_c], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_BIRTH=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DAYS_BIRTH'),\n",
    "                                               boundaries = [-25000,-20000,-15000,-10000])\n",
    "CNT_CHILDREN=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('CNT_CHILDREN'),\n",
    "                                                 boundaries = [2,4,6,8,10])\n",
    "crossed_col_1 = tf.feature_column.crossed_column( [DAYS_BIRTH, CNT_CHILDREN], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_30_CNT_SOCIAL_CIRCLE=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DEF_30_CNT_SOCIAL_CIRCLE'),\n",
    "                                                             boundaries = [2,4,6,8])\n",
    "OBS_30_CNT_SOCIAL_CIRCLE=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('OBS_30_CNT_SOCIAL_CIRCLE'),\n",
    "                                                 boundaries = [5, 10,15,20,25,30])\n",
    "crossed_col_2 = tf.feature_column.crossed_column([DEF_30_CNT_SOCIAL_CIRCLE,OBS_30_CNT_SOCIAL_CIRCLE], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_30_CNT_SOCIAL_CIRCLE=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DEF_30_CNT_SOCIAL_CIRCLE'),\n",
    "                                                            boundaries = [2,4,6,8])\n",
    "OBS_60_CNT_SOCIAL_CIRCLE=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('OBS_60_CNT_SOCIAL_CIRCLE'),\n",
    "                                                 boundaries = [5, 10,15,20,25,30])\n",
    "crossed_col_3 = tf.feature_column.crossed_column([DEF_30_CNT_SOCIAL_CIRCLE,OBS_30_CNT_SOCIAL_CIRCLE], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_REGISTRATION=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DAYS_REGISTRATION'),\n",
    "                                               boundaries = [-25000,-20000,-15000,-10000,-5000,0])\n",
    "DAYS_BIRTH=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DAYS_BIRTH'),\n",
    "                                               boundaries = [-25000,-20000,-15000,-10000])\n",
    "crossed_col_4 = tf.feature_column.crossed_column( [DAYS_BIRTH, DAYS_REGISTRATION], 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
